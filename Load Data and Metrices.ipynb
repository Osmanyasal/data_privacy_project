{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e915917",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = sns.load_dataset('iris')\n",
    "\n",
    "petal_length = iris['petal_length']\n",
    "petal_width = iris['petal_width'] \n",
    "sepal_length = iris['sepal_length']\n",
    "sepal_width = iris['sepal_width']\n",
    "\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59e6536b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchvision'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_13824\\1892212692.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Load the CIFAR-10 dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m cifar10_dataset = torchvision.datasets.CIFAR10(root='path/to/data', \n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torchvision'"
     ]
    }
   ],
   "source": [
    "## Eski\n",
    "\"\"\"\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "# Load the CIFAR-10 dataset\n",
    "cifar10_dataset = torchvision.datasets.CIFAR10(root='path/to/data', \n",
    "                                               train=True, \n",
    "                                               download=True)\n",
    "data, labels = cifar10_dataset[0]\n",
    "\n",
    "# Print the data and labels\n",
    "print(data)\n",
    "print(labels)\n",
    "\n",
    "class_names = torchvision.datasets.CIFAR10.classes\n",
    "print(class_names)\n",
    "\n",
    "label_name = class_names[labels]\n",
    "print(label_name)\n",
    "\"\"\"\n",
    "\n",
    "# Load the raw CIFAR-10 data.\n",
    "cifar10_dir = 'comp411/datasets/cifar-10-batches-py'\n",
    "\n",
    "# Cleaning up variables to prevent loading data multiple times (which may cause memory issue)\n",
    "try:\n",
    "   del X_train, y_train\n",
    "   del X_test, y_test\n",
    "   print('Clear previously loaded data.')\n",
    "except:\n",
    "   pass\n",
    "\n",
    "X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "\n",
    "# As a sanity check, we print out the size of the training and test data.\n",
    "print('Training data shape: ', X_train.shape)\n",
    "print('Training labels shape: ', y_train.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480466e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize some examples from the dataset.\n",
    "# We show a few examples of training images from each class.\n",
    "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "num_classes = len(classes)\n",
    "samples_per_class = 7\n",
    "for y, cls in enumerate(classes):\n",
    "    idxs = np.flatnonzero(y_train == y)\n",
    "    idxs = np.random.choice(idxs, samples_per_class, replace=False)\n",
    "    for i, idx in enumerate(idxs):\n",
    "        plt_idx = i * num_classes + y + 1\n",
    "        plt.subplot(samples_per_class, num_classes, plt_idx)\n",
    "        plt.imshow(X_train[idx].astype('uint8'))\n",
    "        plt.axis('off')\n",
    "        if i == 0:\n",
    "            plt.title(cls)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e975fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train, val, and test sets. In addition we will\n",
    "# create a small development set as a subset of the training data;\n",
    "# we can use this for development so our code runs faster.\n",
    "num_training = 49000\n",
    "num_validation = 1000\n",
    "num_test = 1000\n",
    "num_dev = 500\n",
    "\n",
    "# Our validation set will be num_validation points from the original\n",
    "# training set.\n",
    "mask = range(num_training, num_training + num_validation)\n",
    "X_val = X_train[mask]\n",
    "y_val = y_train[mask]\n",
    "\n",
    "# Our training set will be the first num_train points from the original\n",
    "# training set.\n",
    "mask = range(num_training)\n",
    "X_train = X_train[mask]\n",
    "y_train = y_train[mask]\n",
    "\n",
    "# We will also make a development set, which is a small subset of\n",
    "# the training set.\n",
    "mask = np.random.choice(num_training, num_dev, replace=False)\n",
    "X_dev = X_train[mask]\n",
    "y_dev = y_train[mask]\n",
    "\n",
    "# We use the first num_test points of the original test set as our\n",
    "# test set.\n",
    "mask = range(num_test)\n",
    "X_test = X_test[mask]\n",
    "y_test = y_test[mask]\n",
    "\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1022ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing: reshape the image data into rows\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], -1))\n",
    "X_val = np.reshape(X_val, (X_val.shape[0], -1))\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], -1))\n",
    "X_dev = np.reshape(X_dev, (X_dev.shape[0], -1))\n",
    "\n",
    "# As a sanity check, print out the shapes of the data\n",
    "print('Training data shape: ', X_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('dev data shape: ', X_dev.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decc6d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Preprocessing: subtract the mean image\n",
    "# first: compute the image mean based on the training data\n",
    "mean_image = np.mean(X_train, axis=0)\n",
    "print(mean_image[:10]) # print a few of the elements\n",
    "plt.figure(figsize=(4,4))\n",
    "plt.imshow(mean_image.reshape((32,32,3)).astype('uint8')) # visualize the mean image\n",
    "plt.show()\n",
    "\n",
    "# second: subtract the mean image from train and test data\n",
    "X_train -= mean_image\n",
    "X_val -= mean_image\n",
    "X_test -= mean_image\n",
    "X_dev -= mean_image\n",
    "\n",
    "# third: append the bias dimension of ones (i.e. bias trick) so that our SVM\n",
    "# only has to worry about optimizing a single weight matrix W.\n",
    "X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))])\n",
    "X_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))])\n",
    "X_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))])\n",
    "X_dev = np.hstack([X_dev, np.ones((X_dev.shape[0], 1))])\n",
    "\n",
    "print(X_train.shape, X_val.shape, X_test.shape, X_dev.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "acc62bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Compute the recall metric for a classification model.\n",
    "    \n",
    "    Parameters:\n",
    "    - y_true: ndarray, shape (n_samples,)\n",
    "        True labels.\n",
    "    - y_pred: ndarray, shape (n_samples,)\n",
    "        Predicted labels.\n",
    "        \n",
    "    Returns:\n",
    "    - recall: float\n",
    "        Recall metric.\n",
    "    \"\"\"\n",
    "    # Count true positives\n",
    "    tp = sum((y_true == 1) & (y_pred == 1))\n",
    "    # Count false negatives\n",
    "    fn = sum((y_true == 1) & (y_pred == 0))\n",
    "    # Compute recall\n",
    "    recall = tp / (tp + fn)\n",
    "    return recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ce5b695",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_score(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Compute the F1 score metric for a classification model.\n",
    "    \n",
    "    Parameters:\n",
    "    - y_true: ndarray, shape (n_samples,)\n",
    "        True labels.\n",
    "    - y_pred: ndarray, shape (n_samples,)\n",
    "        Predicted labels.\n",
    "        \n",
    "    Returns:\n",
    "    - f1_score: float\n",
    "        F1 score metric.\n",
    "    \"\"\"\n",
    "    # Count true positives\n",
    "    tp = sum((y_true == 1) & (y_pred == 1))\n",
    "    # Count false positives\n",
    "    fp = sum((y_true == 0) & (y_pred == 1))\n",
    "    # Count false negatives\n",
    "    fn = sum((y_true == 1) & (y_pred == 0))\n",
    "    # Compute precision and recall\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    # Compute F1 score\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "    return f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8eb0f159",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Compute the accuracy metric for a classification model.\n",
    "    \n",
    "    Parameters:\n",
    "    - y_true: ndarray, shape (n_samples,)\n",
    "        True labels.\n",
    "    - y_pred: ndarray, shape (n_samples,)\n",
    "        Predicted labels.\n",
    "        \n",
    "    Returns:\n",
    "    - accuracy: float\n",
    "        Accuracy metric.\n",
    "    \"\"\"\n",
    "    # Count true predictions\n",
    "    n_correct = sum(y_true == y_pred)\n",
    "    # Compute accuracy\n",
    "    accuracy = n_correct / len(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f1022b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Compute the precision metric for a classification model.\n",
    "    \n",
    "    Parameters:\n",
    "    - y_true: ndarray, shape (n_samples,)\n",
    "        True labels.\n",
    "    - y_pred: ndarray, shape (n_samples,)\n",
    "        Predicted labels.\n",
    "        \n",
    "    Returns:\n",
    "    - precision: float\n",
    "        Precision metric.\n",
    "    \"\"\"\n",
    "    # Count true positives\n",
    "    tp = sum((y_true == 1) & (y_pred == 1))\n",
    "    # Count false positives\n",
    "    fp = sum((y_true == 0) & (y_pred == 1))\n",
    "    # Compute precision\n",
    "    precision = tp / (tp + fp)\n",
    "    return precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b204cb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      0.2\n",
       "1      0.2\n",
       "2      0.2\n",
       "3      0.2\n",
       "4      0.2\n",
       "      ... \n",
       "145    2.3\n",
       "146    1.9\n",
       "147    2.0\n",
       "148    2.3\n",
       "149    1.8\n",
       "Name: petal_width, Length: 150, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "petal_width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d6c8fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
